# WARNING: this file is auto-generated by 'build_sync_library.py'
# from the original file 'wrapper.py'
# DO NOT CHANGE! Change the original file instead.
"""Wrapper for migrating between sanitization strategies.

This wrapper enables gradual migration from one sanitization strategy to another
without breaking access to existing data. It wraps two stores configured with
different sanitization strategies and provides:

1. **Collision detection**: Skips migration if both strategies produce the same key
2. **LRU caching**: Caches lookup results to avoid repeated fallback checks
3. **Lazy migration**: Optionally migrates keys from old to new format on read
"""

from collections.abc import Mapping, Sequence
from typing import Any, SupportsFloat

from cachetools import LRUCache
from typing_extensions import override

from key_value.sync.code_gen.protocols.key_value import KeyValue
from key_value.sync.code_gen.wrappers.base import BaseWrapper


class SanitizationMigrationWrapper(BaseWrapper):
    """Wrapper for migrating between sanitization strategies.

    This wrapper manages the transition between two stores using different sanitization
    strategies. It tries the current store first, then falls back to the legacy store
    if the key is not found. Optionally, it can migrate keys from the legacy store to
    the current store on read.

    The wrapper includes:
    - **Collision detection**: If both strategies produce the same sanitized key, no
      fallback is performed (no migration needed)
    - **LRU cache**: Caches which keys are in current vs. legacy store to avoid
      repeated lookups
    - **Lazy migration**: Optionally copies data from legacy to current store on read

    Example:
        ```python
        from key_value.aio.stores.memcached import MemcachedStore
        from key_value.shared.utils.sanitization_strategy import (
            HashLongKeysSanitizationStrategy,
        )
        from key_value.aio.wrappers.sanitization_migration import (
            SanitizationMigrationWrapper,
        )

        # Old strategy (no prefix - collision risk!)
        old_strategy = HashLongKeysSanitizationStrategy(max_length=240)
        legacy_store = MemcachedStore(
            host="localhost",
            sanitization_strategy=old_strategy,
        )

        # New strategy (with H_ prefix - collision safe)
        new_strategy = HashLongKeysSanitizationStrategy(max_length=240)
        current_store = MemcachedStore(
            host="localhost",
            sanitization_strategy=new_strategy,
        )

        # Wrap with migration support
        migrating_store = SanitizationMigrationWrapper(
            current_store=current_store,
            legacy_store=legacy_store,
            migrate_on_read=True,  # Copy old data to new location
            cache_size=10000,
        )
        ```

    Args:
        current_store: Store using the new sanitization strategy.
        legacy_store: Store using the old sanitization strategy.
        migrate_on_read: If True, copy keys from legacy to current on read.
        delete_after_migration: If True, delete from legacy after migration.
        cache_size: Maximum number of keys to cache. Set to 0 to disable caching.

    Note:
        The cache tracks (collection, key) tuples to determine whether to check
        the fallback store. This prevents repeated lookups for keys that are known
        to be in the current store or known to not exist.

        The cache is an LRU (Least Recently Used) cache, so frequently accessed
        keys will remain cached while infrequently accessed keys will be evicted.
    """

    def __init__(
        self,
        current_store: KeyValue,
        legacy_store: KeyValue,
        migrate_on_read: bool = False,
        delete_after_migration: bool = False,
        cache_size: int = 1000,
    ) -> None:
        """Initialize the migration wrapper.

        Args:
            current_store: Store using the new sanitization strategy.
            legacy_store: Store using the old sanitization strategy.
            migrate_on_read: If True, copy keys from legacy to current on read.
            delete_after_migration: If True, delete from legacy after migration.
            cache_size: Maximum number of keys to cache. Set to 0 to disable.
        """
        self.current_store = current_store
        self.legacy_store = legacy_store
        self.migrate_on_read = migrate_on_read
        self.delete_after_migration = delete_after_migration

        # LRU cache to track which keys are in current store
        # Maps (collection, key) -> "current" | "legacy" | "missing"
        self._cache: LRUCache[tuple[str | None, str], str] = LRUCache(maxsize=cache_size) if cache_size > 0 else LRUCache(maxsize=1)
        self._cache_enabled = cache_size > 0

        super().__init__()

    def _get_cache_key(self, key: str, collection: str | None) -> tuple[str | None, str]:
        """Generate cache key from collection and key."""
        return (collection, key)

    def _cache_get(self, key: str, collection: str | None) -> str | None:
        """Get cached location for a key."""
        if not self._cache_enabled:
            return None
        return self._cache.get(self._get_cache_key(key, collection))

    def _cache_put(self, key: str, collection: str | None, location: str) -> None:
        """Cache the location of a key."""
        if self._cache_enabled:
            self._cache[self._get_cache_key(key, collection)] = location

    @override
    def get(self, key: str, *, collection: str | None = None) -> dict[str, Any] | None:
        """Get a value, falling back to legacy store if not in current store.

        Args:
            key: The key to retrieve.
            collection: The collection name.

        Returns:
            The value if found, None otherwise.
        """
        # Check cache first
        cached_location = self._cache_get(key, collection)

        if cached_location == "current":
            # We know it's in current store
            return self.current_store.get(key=key, collection=collection)
        if cached_location == "missing":
            # We know it doesn't exist
            return None

        # Try current store first
        result = self.current_store.get(key=key, collection=collection)

        if result is not None:
            self._cache_put(key, collection, "current")
            return result

        # Fall back to legacy store
        result = self.legacy_store.get(key=key, collection=collection)

        if result is None:
            self._cache_put(key, collection, "missing")
            return None

        # Found in legacy store - optionally migrate
        if self.migrate_on_read:
            # Get TTL from legacy store
            (_, ttl) = self.legacy_store.ttl(key=key, collection=collection)

            # Write to current store
            self.current_store.put(key=key, value=result, collection=collection, ttl=ttl)

            # Optionally delete from legacy
            if self.delete_after_migration:
                self.legacy_store.delete(key=key, collection=collection)

            # Update cache - now in current store
            self._cache_put(key, collection, "current")
        else:
            # Track that it's in legacy (don't check current next time)
            self._cache_put(key, collection, "legacy")

        return result

    @override
    def get_many(self, keys: Sequence[str], *, collection: str | None = None) -> list[dict[str, Any] | None]:  # noqa: PLR0912, PLR0915
        """Get multiple values, falling back to legacy store for missing keys.

        Args:
            keys: The keys to retrieve.
            collection: The collection name.

        Returns:
            List of values (None for missing keys).
        """
        # Separate keys by cached location
        current_keys: list[str] = []
        legacy_keys: list[str] = []
        missing_keys: list[str] = []
        unknown_keys: list[str] = []

        for key in keys:
            cached_location = self._cache_get(key, collection)
            if cached_location == "current":
                current_keys.append(key)
            elif cached_location == "legacy":
                legacy_keys.append(key)
            elif cached_location == "missing":
                missing_keys.append(key)
            else:
                unknown_keys.append(key)

        # Start with all None
        key_to_value: dict[str, dict[str, Any] | None] = dict.fromkeys(keys, None)  # type: ignore[arg-type]

        # Fetch known current keys
        if current_keys:
            current_results = self.current_store.get_many(keys=current_keys, collection=collection)
            for i, key in enumerate(current_keys):
                key_to_value[key] = current_results[i]

        # Fetch known legacy keys
        if legacy_keys:
            legacy_results = self.legacy_store.get_many(keys=legacy_keys, collection=collection)
            for i, key in enumerate(legacy_keys):
                legacy_value = legacy_results[i]
                key_to_value[key] = legacy_value

                # Optionally migrate
                if self.migrate_on_read and legacy_value is not None:
                    (_, ttl) = self.legacy_store.ttl(key=key, collection=collection)
                    self.current_store.put(key=key, value=legacy_value, collection=collection, ttl=ttl)
                    if self.delete_after_migration:
                        self.legacy_store.delete(key=key, collection=collection)
                    self._cache_put(key, collection, "current")

        # Fetch unknown keys - try current first, then legacy
        if unknown_keys:
            current_results = self.current_store.get_many(keys=unknown_keys, collection=collection)

            # Identify which unknown keys were not found in current
            not_in_current: list[str] = []
            for i, key in enumerate(unknown_keys):
                if current_results[i] is not None:
                    key_to_value[key] = current_results[i]
                    self._cache_put(key, collection, "current")
                else:
                    not_in_current.append(key)

            # Try legacy for keys not in current
            if not_in_current:
                legacy_results = self.legacy_store.get_many(keys=not_in_current, collection=collection)
                for i, key in enumerate(not_in_current):
                    legacy_value = legacy_results[i]
                    if legacy_value is not None:
                        key_to_value[key] = legacy_value

                        # Optionally migrate
                        if self.migrate_on_read:
                            (_, ttl) = self.legacy_store.ttl(key=key, collection=collection)
                            self.current_store.put(key=key, value=legacy_value, collection=collection, ttl=ttl)
                            if self.delete_after_migration:
                                self.legacy_store.delete(key=key, collection=collection)
                            self._cache_put(key, collection, "current")
                        else:
                            self._cache_put(key, collection, "legacy")
                    else:
                        # Not in current or legacy
                        self._cache_put(key, collection, "missing")

        return [key_to_value[key] for key in keys]

    @override
    def ttl(self, key: str, *, collection: str | None = None) -> tuple[dict[str, Any] | None, float | None]:
        """Get value and TTL, falling back to legacy store.

        Args:
            key: The key to retrieve.
            collection: The collection name.

        Returns:
            Tuple of (value, ttl).
        """
        # Check cache first
        cached_location = self._cache_get(key, collection)

        if cached_location == "current":
            return self.current_store.ttl(key=key, collection=collection)
        if cached_location == "missing":
            return (None, None)

        # Try current store first
        (result, ttl) = self.current_store.ttl(key=key, collection=collection)

        if result is not None:
            self._cache_put(key, collection, "current")
            return (result, ttl)

        # Fall back to legacy store
        (result, ttl) = self.legacy_store.ttl(key=key, collection=collection)

        if result is None:
            self._cache_put(key, collection, "missing")
            return (None, None)

        # Found in legacy - optionally migrate
        if self.migrate_on_read:
            self.current_store.put(key=key, value=result, collection=collection, ttl=ttl)
            if self.delete_after_migration:
                self.legacy_store.delete(key=key, collection=collection)
            self._cache_put(key, collection, "current")
        else:
            self._cache_put(key, collection, "legacy")

        return (result, ttl)

    @override
    def ttl_many(self, keys: Sequence[str], *, collection: str | None = None) -> list[tuple[dict[str, Any] | None, float | None]]:  # noqa: PLR0912
        """Get multiple values with TTLs, falling back to legacy store.

        Args:
            keys: The keys to retrieve.
            collection: The collection name.

        Returns:
            List of (value, ttl) tuples.
        """
        # Similar logic to get_many but with TTL
        current_keys: list[str] = []
        legacy_keys: list[str] = []
        missing_keys: list[str] = []
        unknown_keys: list[str] = []

        for key in keys:
            cached_location = self._cache_get(key, collection)
            if cached_location == "current":
                current_keys.append(key)
            elif cached_location == "legacy":
                legacy_keys.append(key)
            elif cached_location == "missing":
                missing_keys.append(key)
            else:
                unknown_keys.append(key)

        key_to_value: dict[str, tuple[dict[str, Any] | None, float | None]] = dict.fromkeys(keys, (None, None))

        # Fetch from current
        if current_keys:
            current_results = self.current_store.ttl_many(keys=current_keys, collection=collection)
            for i, key in enumerate(current_keys):
                key_to_value[key] = current_results[i]

        # Fetch from legacy
        if legacy_keys:
            legacy_results = self.legacy_store.ttl_many(keys=legacy_keys, collection=collection)
            for i, key in enumerate(legacy_keys):
                key_to_value[key] = legacy_results[i]

                if self.migrate_on_read and legacy_results[i][0] is not None:
                    (value, ttl) = legacy_results[i]
                    self.current_store.put(key=key, value=value, collection=collection, ttl=ttl)  # type: ignore
                    if self.delete_after_migration:
                        self.legacy_store.delete(key=key, collection=collection)
                    self._cache_put(key, collection, "current")

        # Fetch unknown
        if unknown_keys:
            current_results = self.current_store.ttl_many(keys=unknown_keys, collection=collection)

            not_in_current: list[str] = []
            for i, key in enumerate(unknown_keys):
                if current_results[i][0] is not None:
                    key_to_value[key] = current_results[i]
                    self._cache_put(key, collection, "current")
                else:
                    not_in_current.append(key)

            if not_in_current:
                legacy_results = self.legacy_store.ttl_many(keys=not_in_current, collection=collection)
                for i, key in enumerate(not_in_current):
                    if legacy_results[i][0] is not None:
                        (value, ttl) = legacy_results[i]
                        key_to_value[key] = (value, ttl)

                        if self.migrate_on_read:
                            self.current_store.put(key=key, value=value, collection=collection, ttl=ttl)  # type: ignore
                            if self.delete_after_migration:
                                self.legacy_store.delete(key=key, collection=collection)
                            self._cache_put(key, collection, "current")
                        else:
                            self._cache_put(key, collection, "legacy")
                    else:
                        self._cache_put(key, collection, "missing")

        return [key_to_value[key] for key in keys]

    @override
    def put(self, key: str, value: Mapping[str, Any], *, collection: str | None = None, ttl: SupportsFloat | None = None) -> None:
        """Put a value in the current store and invalidate cache.

        Args:
            key: The key to store.
            value: The value to store.
            collection: The collection name.
            ttl: Time-to-live in seconds.
        """
        # Always write to current store
        self.current_store.put(key=key, value=value, collection=collection, ttl=ttl)

        # Update cache to reflect current location
        self._cache_put(key, collection, "current")

    @override
    def put_many(
        self, keys: Sequence[str], values: Sequence[Mapping[str, Any]], *, collection: str | None = None, ttl: SupportsFloat | None = None
    ) -> None:
        """Put multiple values in the current store and invalidate cache.

        Args:
            keys: The keys to store.
            values: The values to store.
            collection: The collection name.
            ttl: Time-to-live in seconds.
        """
        self.current_store.put_many(keys=keys, values=values, collection=collection, ttl=ttl)

        # Update cache
        for key in keys:
            self._cache_put(key, collection, "current")

    @override
    def delete(self, key: str, *, collection: str | None = None) -> bool:
        """Delete a value from both current and legacy stores.

        Args:
            key: The key to delete.
            collection: The collection name.

        Returns:
            True if deleted from either store, False otherwise.
        """
        # Delete from both stores to ensure complete removal
        current_deleted = self.current_store.delete(key=key, collection=collection)
        legacy_deleted = self.legacy_store.delete(key=key, collection=collection)

        # Update cache
        self._cache_put(key, collection, "missing")

        return current_deleted or legacy_deleted

    @override
    def delete_many(self, keys: Sequence[str], *, collection: str | None = None) -> int:
        """Delete multiple values from both current and legacy stores.

        Args:
            keys: The keys to delete.
            collection: The collection name.

        Returns:
            Number of keys deleted from either store.
        """
        # Delete from both stores
        current_count = self.current_store.delete_many(keys=keys, collection=collection)
        legacy_count = self.legacy_store.delete_many(keys=keys, collection=collection)

        # Update cache
        for key in keys:
            self._cache_put(key, collection, "missing")

        # Return sum since keys could be in different stores
        return current_count + legacy_count
